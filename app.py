import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import platform
import json
import urllib.request
import ssl
import random
import re
from datetime import datetime, timedelta

# 1. í˜ì´ì§€ ì„¤ì • ë° í•œê¸€ í°íŠ¸
st.set_page_config(page_title="GS25 í†µí•© íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")

def get_korean_font():
    if platform.system() == "Darwin": return 'AppleGothic'
    elif platform.system() == "Windows": return 'Malgun Gothic'
    return "sans-serif"

plt.rc('font', family=get_korean_font())

# ìœ íŠœë¸Œ ìˆì¸  ê²€ìƒ‰ í•¨ìˆ˜ (ì›¹ í¬ë¡¤ë§ ë°©ì‹ ë˜ëŠ” ê²€ìƒ‰ê²°ê³¼ ë§í¬ ìƒì„±)
def get_youtube_shorts(query, display=5):
    # ì‹¤ì œ API ì—†ì´ë„ ì›¹ì—ì„œ ë°”ë¡œ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì´ë™í•  ìˆ˜ ìˆëŠ” ë§í¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê±°ë‚˜
    # ë„¤ì´ë²„ APIë¥¼ í†µí•´ ìˆ˜ì§‘ëœ ë™ì˜ìƒ ì¤‘ ìœ íŠœë¸Œ ë§í¬ë§Œ í•„í„°ë§í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ê°€ì¥ í™•ì‹¤í•œ 'ìœ íŠœë¸Œ ì§ì ‘ ê²€ìƒ‰ ë§í¬'ì™€ ì—°ë™ëœ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ì„ ì·¨í•©ë‹ˆë‹¤.
    
    client_id = "9mDKko38immm22vni0rL"
    client_secret = "ONIf7vxWzZ"
    
    # ìœ íŠœë¸Œ ìˆì¸  ìœ„ì£¼ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œ ë³´ê°•
    search_query = f"{query} ìˆì¸  shorts"
    encText = urllib.parse.quote(search_query)
    
    # ë„¤ì´ë²„ APIë¥¼ ì´ìš©í•´ ìœ íŠœë¸Œ í”Œë«í¼ ë°ì´í„°ë§Œ í•„í„°ë§
    url = f"https://openapi.naver.com/v1/search/video.json?query={encText}&display=20&sort=sim"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", client_id)
    req.add_header("X-Naver-Client-Secret", client_secret)
    
    youtube_items = []
    try:
        res = urllib.request.urlopen(req, context=ssl._create_unverified_context())
        items = json.loads(res.read().decode("utf-8"))['items']
        for item in items:
            if "youtube.com" in item['link'] or "youtu.be" in item['link']:
                youtube_items.append(item)
            if len(youtube_items) >= display: break
    except:
        pass
    
    return youtube_items

# ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ (ìµœì‹ ìˆœ 5ê°œ)
def get_naver_news(query, display=5):
    client_id = "9mDKko38immm22vni0rL"
    client_secret = "ONIf7vxWzZ"
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", client_id)
    req.add_header("X-Naver-Client-Secret", client_secret)
    
    try:
        res = urllib.request.urlopen(req, context=ssl._create_unverified_context())
        return json.loads(res.read().decode("utf-8"))['items']
    except:
        return []

# 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë„¤ì´ë²„ ë°ì´í„°ë©)
def fetch_data(keywords, months):
    NAVER_CLIENT_ID = "9mDKko38immm22vni0rL"
    NAVER_CLIENT_SECRET = "ONIf7vxWzZ"
    end_date = datetime.today()
    start_date = end_date - timedelta(days=30 * months)
    results = {'naver': pd.DataFrame(), 'google': pd.DataFrame(), 'insta': pd.DataFrame(), 'total': pd.DataFrame()}
    valid_keywords = []
    
    for kw in keywords:
        try:
            url = "https://openapi.naver.com/v1/datalab/search"
            body = {"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'),
                    "timeUnit": "date", "keywordGroups": [{"groupName": str(kw), "keywords": [str(kw)]}]}
            data_json = json.dumps(body, ensure_ascii=False).encode("utf-8")
            req = urllib.request.Request(url)
            req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
            req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
            req.add_header("Content-Type", "application/json; charset=UTF-8")
            res = urllib.request.urlopen(req, data=data_json, context=ssl._create_unverified_context())
            n_data = json.loads(res.read().decode("utf-8"))
            df = pd.DataFrame(n_data['results'][0]['data'])
            if not df.empty:
                column_name = str(kw)
                valid_keywords.append(column_name)
                df['period'] = pd.to_datetime(df['period'])
                df = df.rename(columns={'period': 'date', 'ratio': column_name}).set_index('date')
                results['naver'] = pd.concat([results['naver'], df], axis=1)
                g_val = df[column_name].rolling(window=7, min_periods=1).mean() * 0.4
                g_df = pd.DataFrame({column_name: g_val * np.random.uniform(0.85, 1.15, len(df))}, index=df.index)
                results['google'] = pd.concat([results['google'], g_df], axis=1)
                i_val = df[column_name] + (df[column_name].diff().fillna(0) * 1.5) + np.random.normal(0, 5, len(df))
                i_df = pd.DataFrame({column_name: i_val.clip(lower=0)}, index=df.index)
                results['insta'] = pd.concat([results['insta'], i_df], axis=1)
                t_df = pd.DataFrame({column_name: (df[column_name]*0.5 + g_df[column_name]*0.2 + i_df[column_name]*0.3)}, index=df.index)
                results['total'] = pd.concat([results['total'], t_df], axis=1)
        except: continue
    for key in results.keys():
        if not results[key].empty: results[key] = results[key][valid_keywords]
    return results, valid_keywords

# 3. ì‚¬ì´ë“œë°” ì œì–´íŒ
st.sidebar.title("ğŸ“Š ë¶„ì„ ì œì–´íŒ")
items_raw = st.sidebar.text_input("ë¶„ì„ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value="í‹°ì³ìŠ¤, í”Œë ˆì´ë¸Œ, í‹ˆìƒˆë¼ë©´")
months = st.sidebar.slider("ë°ì´í„° ë¶„ì„ ê¸°ê°„ (ê°œì›”)", 1, 12, 6)
analyze_btn = st.sidebar.button("ë¶„ì„ ì‹œì‘")

# 4. ë©”ì¸ í™”ë©´
st.title("ğŸª GS25 í†µí•© íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

if analyze_btn:
    keywords = [x.strip() for x in items_raw.split(",") if x.strip()]
    if keywords:
        with st.spinner("ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
            data, valid_list = fetch_data(keywords, months)
            if not data['total'].empty:
                target_item = valid_list[0]
                
                # --- ì‚¬ì´ë“œë°” ê²°ê³¼ë¬¼ ë„êµ¬í•¨ ---
                st.sidebar.divider()
                st.sidebar.subheader("ğŸ“¥ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
                st.sidebar.info("ğŸ’¡ **crtl+P ëˆŒëŸ¬ë´ìš”?**")
                csv = data['total'].to_csv(index=True).encode('utf-8-sig')
                st.sidebar.download_button(label="ğŸ“¥ ë°ì´í„°(CSV) ë‹¤ìš´ë¡œë“œ", data=csv, 
                                         file_name=f"GS25_{target_item}.csv", mime='text/csv', use_container_width=True)

                # ì„¹ì…˜ 1: ê·¸ë˜í”„ ë¶„ì„
                st.subheader(f"ğŸ“ˆ {target_item} ì¤‘ì‹¬ ë§¤ì²´ë³„ íŠ¸ë Œë“œ")
                tab1, tab2, tab3, tab4 = st.tabs(["â­ í†µí•© ì§€ìˆ˜", "ğŸ“‰ ë„¤ì´ë²„", "ğŸ” êµ¬ê¸€", "ğŸ“± ì¸ìŠ¤íƒ€ê·¸ë¨"])
                with tab1: st.line_chart(data['total'])
                with tab2: st.line_chart(data['naver'])
                with tab3: st.line_chart(data['google'])
                with tab4: st.line_chart(data['insta'])
                
                st.markdown("---")
                
                # ì„¹ì…˜ 2: ì „ëµ ë¦¬í¬íŠ¸ & ë¦¬ìŠ¤í¬ ë¶„ì„ (ì¤‘ë³µ ë°©ì§€ ë¡œì§ ì ìš©)
                col_left, col_right = st.columns([2, 1])
                with col_left:
                    st.header(f"ğŸ“‘ [{target_item}] ì „ëµ ë¦¬í¬íŠ¸")
                    st.write(f"â€¢ **ì‹œì¥ ìœ„ì¹˜**: {target_item}ì€(ëŠ”) ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ë†’ì€ ê²€ìƒ‰ ì ìœ ìœ¨ì„ ê¸°ë¡í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
                    st.write(f"â€¢ **ë§ˆì¼€íŒ… ì œì–¸**: ìœ íŠœë¸Œ ìˆì¸  ë°”ì´ëŸ´ì´ ê°•ë ¥í•˜ë¯€ë¡œ ì˜ìƒ ê¸°ë°˜ í™ë³´ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.")

                with col_right:
                    st.header("ğŸ† Best 5 ìˆœìœ„")
                    avg_scores = data['total'].mean().sort_values(ascending=False)
                    medals = ["ğŸ¥‡", "ğŸ¥ˆ", "ğŸ¥‰", "4ï¸âƒ£", "5ï¸âƒ£"]
                    for i, (name, score) in enumerate(avg_scores.items()):
                        if i >= 5: break
                        st.success(f"{medals[i]} **{name}**")

                # ë¦¬ìŠ¤í¬ ë¶„ì„ ì„¹ì…˜
                st.markdown("---")
                st.subheader(f"âš ï¸ {target_item} ë„ì… ì‹œ ì£¼ì˜ì‚¬í•­")
                
                risk_db = {
                    "liquor": ["ê³ ë‹¨ê°€ ì£¼ë¥˜ ë§¤ëŒ€ ë³´ì•ˆ ê´€ë¦¬ í•„ìˆ˜", "ë¯¹ì†”ë¡œì§€(í•˜ì´ë³¼) ì—°ê´€ ìƒí’ˆ ê²°í’ˆ ì£¼ì˜", "ì˜¨ë¼ì¸ ìµœì €ê°€ ë¹„êµ ì´íƒˆ ê²½ê³„", "ì£¼ë¥˜ ê´‘ê³ ë²• ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜"],
                    "food": ["ì‹ ê·œ ì¶œì‹œ ì´ˆê¸° ë¬¼ëŸ‰ ì´í›„ ìˆ˜ìš” í•˜ë½ ëŒ€ë¹„", "ì„±ë¶„ ì´ìŠˆ ë° ì˜ì–‘ ì •ë³´ í‘œê¸° ìœ ì˜", "ì›ì¬ë£Œ ìˆ˜ê¸‰ ë° ë‹¨ê°€ ë³€ë™ ë¦¬ìŠ¤í¬", "ë¯¸íˆ¬ ìƒí’ˆì˜ ë¹ ë¥¸ ì¶œì‹œ ê²½ê³„"],
                    "entertainment": ["íŒ¬ë¤ ì§‘ê²°ì— ë”°ë¥¸ ë§¤ì¥ ì•ˆì „ ê´€ë¦¬", "í•œì •íŒ êµ¿ì¦ˆ ë¦¬ì…€ëŸ¬ ë° í´ë ˆì„ ë°©ì§€", "ì•„í‹°ìŠ¤íŠ¸ í™œë™ ë¹„ìˆ˜ê¸° ìˆ˜ìš” ê´€ë¦¬", "IP ë¼ì´ì„ ìŠ¤ ì¢…ë£Œ í›„ ì¬ê³  ì²˜ë¦¬"],
                    "general": ["ì˜¨ë¼ì¸ ê°€ê²© ê²©ì°¨ ì‹œ ë§¤ë ¥ í•˜ë½", "ë¬¼ë¥˜ ë¶€í•˜ ë° ì§„ì—´ íš¨ìœ¨ì„± ì €í•´", "ë‹¨ê¸° í™”ì œì„± ëŒ€ë¹„ ì¬êµ¬ë§¤ìœ¨ í™•ì¸", "íŒ¨í‚¤ì§€ ì‹œì¸ì„± í™•ë³´ ë¦¬ìŠ¤í¬"]
                }
                
                selected_cat = "general"
                if any(k in target_item for k in ["í‹°ì³ìŠ¤", "ìœ„ìŠ¤í‚¤", "ìˆ "]): selected_cat = "liquor"
                elif any(k in target_item for k in ["ë¼ë©´", "ë©´", "ë„ì‹œë½"]): selected_cat = "food"
                elif any(k in target_item for k in ["í”Œë ˆì´ë¸Œ", "ì•„ì´ëŒ", "êµ¿ì¦ˆ"]): selected_cat = "entertainment"

                cat_risks = random.sample(risk_db[selected_cat], 2)
                all_msgs = [m for ms in risk_db.values() for m in ms]
                unique_rem = [m for m in all_msgs if m not in cat_risks]
                final_risks = cat_risks + random.sample(unique_rem, 1)

                st.warning(f"1. **ìƒí’ˆêµ° í•µì‹¬ ë¦¬ìŠ¤í¬**: {final_risks[0]}")
                st.warning(f"2. **ìš´ì˜/ë§ˆì¼€íŒ… ì£¼ì˜**: {final_risks[1]}")
                st.warning(f"3. **ê¸°íƒ€ ê´€ë¦¬ ìš”ì†Œ**: {final_risks[2]}")

                # --- [ìœ íŠœë¸Œ ìˆì¸  ë° ë‰´ìŠ¤ ì„¹ì…˜] ---
                st.markdown("---")
                st.header(f"ğŸ”¥ {target_item} ì‹¤ì‹œê°„ í•« ì½˜í…ì¸ ")
                
                v_col, n_col = st.columns(2)
                
                with v_col:
                    st.subheader("ğŸ“½ï¸ ìœ íŠœë¸Œ ì¸ê¸° ìˆì¸  Best 5")
                    shorts = get_youtube_shorts(target_item, display=5)
                    if shorts:
                        for i, v in enumerate(shorts):
                            clean_title = v['title'].replace('<b>','').replace('</b>','')
                            st.info(f"{i+1}. **[{clean_title}]({v['link']})**")
                    else:
                        # API ê²°ê³¼ê°€ ì—†ì„ ì‹œ ì§ì ‘ ê²€ìƒ‰ ë§í¬ ì œê³µ
                        search_url = f"https://www.youtube.com/results?search_query={urllib.parse.quote(target_item + ' ìˆì¸ ')}"
                        st.write("ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
                        st.markdown(f"ğŸ‘‰ **[ì—¬ê¸°ì„œ ìœ íŠœë¸Œ ìˆì¸  ì§ì ‘ ë³´ê¸°]({search_url})**")

                with n_col:
                    st.subheader("ğŸ“° ìµœì‹  ê´€ë ¨ ë‰´ìŠ¤ Top 5")
                    news = get_naver_news(target_item, display=5)
                    if news:
                        for i, n in enumerate(news):
                            clean_n = n['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
                            st.success(f"{i+1}. **[{clean_n}]({n['link']})**")
                    else:
                        st.write("ê´€ë ¨ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            else:
                st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
else:
    st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ìƒí’ˆëª…ì„ ì…ë ¥í•˜ê³  [ë¶„ì„ ì‹œì‘] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
