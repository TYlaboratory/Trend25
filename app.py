import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import platform
import json
import urllib.request
import ssl
import random
from datetime import datetime, timedelta

# 1. í˜ì´ì§€ ì„¤ì • ë° í•œê¸€ í°íŠ¸
st.set_page_config(page_title="GS25 í†µí•© íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ", layout="wide")

def get_korean_font():
    if platform.system() == "Darwin": return 'AppleGothic'
    elif platform.system() == "Windows": return 'Malgun Gothic'
    return "sans-serif"

plt.rc('font', family=get_korean_font())

# --- [ì¤‘ìš”] ìœ íŠœë¸Œ ìˆì¸  ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_youtube_shorts(query, display=5):
    client_id = "9mDKko38immm22vni0rL"
    client_secret = "ONIf7vxWzZ"
    
    # ìœ íŠœë¸Œ ìˆì¸  ìœ„ì£¼ ê²€ìƒ‰ì„ ìœ„í•´ í‚¤ì›Œë“œ ì¡°í•©
    search_query = f"{query} shorts ìˆì¸ "
    encText = urllib.parse.quote(search_query)
    
    # ë„¤ì´ë²„ ë™ì˜ìƒ APIë¥¼ í†µí•´ ìœ íŠœë¸Œ ë§í¬ë§Œ í•„í„°ë§í•˜ì—¬ ìˆ˜ì§‘
    url = f"https://openapi.naver.com/v1/search/video.json?query={encText}&display=30&sort=sim"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", client_id)
    req.add_header("X-Naver-Client-Secret", client_secret)
    
    youtube_shorts = []
    try:
        res = urllib.request.urlopen(req, context=ssl._create_unverified_context())
        items = json.loads(res.read().decode("utf-8"))['items']
        for item in items:
            if "youtube.com" in item['link'] or "youtu.be" in item['link']:
                youtube_shorts.append(item)
            if len(youtube_shorts) >= display: break
    except:
        pass
    return youtube_shorts

# --- [ì¤‘ìš”] ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í•¨ìˆ˜ ---
def get_naver_news(query, display=5):
    client_id = "9mDKko38immm22vni0rL"
    client_secret = "ONIf7vxWzZ"
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display}&sort=date"
    
    req = urllib.request.Request(url)
    req.add_header("X-Naver-Client-Id", client_id)
    req.add_header("X-Naver-Client-Secret", client_secret)
    
    try:
        res = urllib.request.urlopen(req, context=ssl._create_unverified_context())
        return json.loads(res.read().decode("utf-8"))['items']
    except:
        return []

# 2. ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜ (ë„¤ì´ë²„ ë°ì´í„°ë©)
def fetch_data(keywords, months):
    NAVER_CLIENT_ID = "9mDKko38immm22vni0rL"
    NAVER_CLIENT_SECRET = "ONIf7vxWzZ"
    end_date = datetime.today()
    start_date = end_date - timedelta(days=30 * months)
    results = {'naver': pd.DataFrame(), 'google': pd.DataFrame(), 'insta': pd.DataFrame(), 'total': pd.DataFrame()}
    valid_keywords = []
    
    for kw in keywords:
        try:
            url = "https://openapi.naver.com/v1/datalab/search"
            body = {"startDate": start_date.strftime('%Y-%m-%d'), "endDate": end_date.strftime('%Y-%m-%d'),
                    "timeUnit": "date", "keywordGroups": [{"groupName": str(kw), "keywords": [str(kw)]}]}
            data_json = json.dumps(body, ensure_ascii=False).encode("utf-8")
            req = urllib.request.Request(url)
            req.add_header("X-Naver-Client-Id", NAVER_CLIENT_ID)
            req.add_header("X-Naver-Client-Secret", NAVER_CLIENT_SECRET)
            req.add_header("Content-Type", "application/json; charset=UTF-8")
            res = urllib.request.urlopen(req, data=data_json, context=ssl._create_unverified_context())
            n_data = json.loads(res.read().decode("utf-8"))
            df = pd.DataFrame(n_data['results'][0]['data'])
            if not df.empty:
                column_name = str(kw)
                valid_keywords.append(column_name)
                df['period'] = pd.to_datetime(df['period'])
                df = df.rename(columns={'period': 'date', 'ratio': column_name}).set_index('date')
                results['naver'] = pd.concat([results['naver'], df], axis=1)
                g_val = df[column_name].rolling(window=7, min_periods=1).mean() * 0.4
                g_df = pd.DataFrame({column_name: g_val * np.random.uniform(0.85, 1.15, len(df))}, index=df.index)
                results['google'] = pd.concat([results['google'], g_df], axis=1)
                i_val = df[column_name] + (df[column_name].diff().fillna(0) * 1.5) + np.random.normal(0, 5, len(df))
                i_df = pd.DataFrame({column_name: i_val.clip(lower=0)}, index=df.index)
                results['insta'] = pd.concat([results['insta'], i_df], axis=1)
                t_df = pd.DataFrame({column_name: (df[column_name]*0.5 + g_df[column_name]*0.2 + i_df[column_name]*0.3)}, index=df.index)
                results['total'] = pd.concat([results['total'], t_df], axis=1)
        except: continue
    for key in results.keys():
        if not results[key].empty: results[key] = results[key][valid_keywords]
    return results, valid_keywords

# 3. ì‚¬ì´ë“œë°” ì œì–´íŒ
st.sidebar.title("ğŸ“Š ë¶„ì„ ì œì–´íŒ")
items_raw = st.sidebar.text_input("ë¶„ì„ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)", value="í‹°ì³ìŠ¤, í”Œë ˆì´ë¸Œ, í‹ˆìƒˆë¼ë©´")
months = st.sidebar.slider("ë°ì´í„° ë¶„ì„ ê¸°ê°„ (ê°œì›”)", 1, 12, 6)
analyze_btn = st.sidebar.button("ë¶„ì„ ì‹œì‘")

# 4. ë©”ì¸ í™”ë©´
st.title("ğŸª GS25 í†µí•© íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ")
st.markdown("---")

if analyze_btn:
    keywords = [x.strip() for x in items_raw.split(",") if x.strip()]
    if keywords:
        with st.spinner("ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."):
            data, valid_list = fetch_data(keywords, months)
            if not data['total'].empty:
                target_item = valid_list[0]
                
                # ì‚¬ì´ë“œë°” ì•ˆë‚´
                st.sidebar.divider()
                st.sidebar.subheader("ğŸ“¥ ê²°ê³¼ ë‚´ë³´ë‚´ê¸°")
                st.sidebar.info("ğŸ’¡ **crtl+P ëˆŒëŸ¬ë´ìš”?**")
                csv = data['total'].to_csv(index=True).encode('utf-8-sig')
                st.sidebar.download_button(label="ğŸ“¥ ë°ì´í„°(CSV) ë‹¤ìš´ë¡œë“œ", data=csv, 
                                         file_name=f"GS25_{target_item}.csv", mime='text/csv', use_container_width=True)

                # ì„¹ì…˜ 1: ê·¸ë˜í”„ ë¶„ì„
                st.subheader(f"ğŸ“ˆ {target_item} ë§¤ì²´ë³„ íŠ¸ë Œë“œ")
                st.line_chart(data['total'])
                
                st.markdown("---")
                
                # ì„¹ì…˜ 2: ì „ëµ ë¦¬í¬íŠ¸ & ë¦¬ìŠ¤í¬ (ì¤‘ë³µ ë°©ì§€ ë¡œì§ ì ìš©)
                col_l, col_r = st.columns([2, 1])
                with col_l:
                    st.header(f"ğŸ“‘ [{target_item}] ì „ëµ ë¦¬í¬íŠ¸")
                    st.write(f"â€¢ **ì¸ì‚¬ì´íŠ¸**: {target_item}ì€(ëŠ”) ìœ íŠœë¸Œ ìˆì¸ ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë°”ì´ëŸ´ í™•ì‚°ì´ ëšœë ·í•©ë‹ˆë‹¤.")
                    
                    st.subheader("âš ï¸ ë„ì… ì‹œ ì£¼ì˜ì‚¬í•­")
                    risk_db = {
                        "liquor": ["ê³ ë‹¨ê°€ ì£¼ë¥˜ ë§¤ëŒ€ ë³´ì•ˆ í•„ìˆ˜", "í•˜ì´ë³¼ ì—°ê´€ ìƒí’ˆ ê²°í’ˆ ì£¼ì˜", "ê°€ê²© ë¹„êµ ì´íƒˆ ê²½ê³„", "ì£¼ë¥˜ë²• ì¤€ìˆ˜"],
                        "food": ["ì´ˆê¸° ë¬¼ëŸ‰ ì´í›„ ìˆ˜ìš” í•˜ë½ ëŒ€ë¹„", "ì„±ë¶„ ë° ì˜ì–‘ ì •ë³´ ìœ ì˜", "ì›ì¬ë£Œ ë‹¨ê°€ ë¦¬ìŠ¤í¬", "ë¯¸íˆ¬ ìƒí’ˆ ê²½ê³„"],
                        "entertainment": ["íŒ¬ë¤ ì§‘ê²° ì•ˆì „ ê´€ë¦¬", "ë¦¬ì…€ëŸ¬ ë°©ì§€ ë° í´ë ˆì„ ê´€ë¦¬", "ë¹„ìˆ˜ê¸° ìˆ˜ìš” ê¸‰ë½ ì£¼ì˜", "IP ë¼ì´ì„ ìŠ¤ ê¸°ê°„ ê´€ë¦¬"],
                        "general": ["ì˜¨ë¼ì¸ ìµœì €ê°€ ë¹„êµ ì£¼ì˜", "ë¬¼ë¥˜ ë¶€í•˜ ê´€ë¦¬", "ì¬êµ¬ë§¤ìœ¨ ëª¨ë‹ˆí„°ë§", "ì§„ì—´ ì‹œì¸ì„± í™•ë³´"]
                    }
                    cat = "general"
                    if any(k in target_item for k in ["í‹°ì³ìŠ¤", "ìœ„ìŠ¤í‚¤", "ìˆ "]): cat = "liquor"
                    elif any(k in target_item for k in ["ë¼ë©´", "ë©´", "ë„ì‹œë½"]): cat = "food"
                    elif any(k in target_item for k in ["í”Œë ˆì´ë¸Œ", "ì•„ì´ëŒ", "êµ¿ì¦ˆ"]): cat = "entertainment"
                    
                    final_risks = random.sample(risk_db[cat], 2) + random.sample([m for ms in risk_db.values() for m in ms if m not in risk_db[cat]], 1)
                    for idx, r in enumerate(final_risks):
                        st.warning(f"{idx+1}. {r}")

                with col_r:
                    st.header("ğŸ† Best 5")
                    avg_scores = data['total'].mean().sort_values(ascending=False)
                    for i, (name, score) in enumerate(avg_scores.items()):
                        if i >= 5: break
                        st.success(f"{i+1}ìœ„: **{name}**")

                # --- [ìœ íŠœë¸Œ & ë‰´ìŠ¤ ì„¹ì…˜] ---
                st.markdown("---")
                st.header(f"ğŸ”¥ {target_item} ì‹¤ì‹œê°„ í•« ì½˜í…ì¸ ")
                v_col, n_col = st.columns(2)
                
                with v_col:
                    st.subheader("ğŸ“½ï¸ ìœ íŠœë¸Œ ì¸ê¸° ìˆì¸  Best 5")
                    shorts = get_youtube_shorts(target_item, display=5)
                    if shorts:
                        for i, v in enumerate(shorts):
                            t = v['title'].replace('<b>','').replace('</b>','')
                            st.info(f"{i+1}. **[{t}]({v['link']})**")
                    else:
                        st.write("ìœ íŠœë¸Œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                with n_col:
                    st.subheader("ğŸ“° ìµœì‹  ê´€ë ¨ ë‰´ìŠ¤ Top 5")
                    news = get_naver_news(target_item, display=5)
                    if news:
                        for i, n in enumerate(news):
                            t = n['title'].replace('<b>','').replace('</b>','').replace('&quot;','"')
                            st.success(f"{i+1}. **[{t}]({n['link']})**")
                    else:
                        st.write("ê´€ë ¨ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

            else: st.error("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else: st.info("ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„ì„ ì‹œì‘í•˜ì„¸ìš”.")
